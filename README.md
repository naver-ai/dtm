## [Morphing Tokens Draw Strong Masked Image Models](https://arxiv.org/abs/2401.00254)

> [Taekyung Kim*](https://scholar.google.co.kr/citations?user=u-9bdkwAAAAJ&hl=en), [Byeongho Heo](https://sites.google.com/view/byeongho-heo/home), [Dongyoon Han*](https://sites.google.com/site/dyhan0920/) <br>
> <sub> (*equal contribution) <br> </sub>
> [NAVER AI LAB](https://naver-career.gitbook.io/en/teams/clova-cic/ai-lab)

[![paper](https://img.shields.io/badge/arXiv-Paper-red.svg)](https://arxiv.org/abs/2401.00254)


### Abstract
> Masked image modeling (MIM) has emerged as a promising approach for training Vision Transformers (ViTs). The essence of MIM lies in the token-wise prediction of masked tokens, which aims to predict targets tokenized from images or generated by pre-trained models like vision-language models. While using tokenizers or pre-trained models are plausible MIM targets, they often offer spatially inconsistent targets even for neighboring tokens, complicating models to learn unified and discriminative representations. Our pilot study identifies spatial inconsistencies and suggests that resolving them can accelerate representation learning. Building upon this insight, we introduce a novel self-supervision signal called Dynamic Token Morphing (DTM), which dynamically aggregates contextually related tokens to yield contextualized targets, thereby mitigating spatial inconsistency. DTM is compatible with various SSL frameworks; we showcase improved MIM results by employing DTM, barely introducing extra training costs. Our method facilitates training by using consistent targets, resulting in 1) faster training and 2) reduced losses. Experiments on ImageNet-1K and ADE20K demonstrate the superiority of our method compared with state-of-the-art, complex MIM methods. Furthermore, the comparative evaluation of the iNaturalists and fine-grained visual classification datasets further validates the transferability of our method on various downstream tasks.


## Contents
- [Motivation & Method Description](#motivation--method-description)
- [Install](#install)
- [Dataset](#dataset)
- [Train](docs/Training.md)
- [Evaluation](docs/Evaluation.md)

## Motivation & Method Description
### Our Motivation: What is spatial consistency among visual tokens?
- (a): input image; (b) and (c) display the predicted classes for each token within 4 example bounding boxes without/with token aggregations, respectively.
- Shades of red and green represent the degree of incorrect and correct predictions, respectively.
- The zero-shot accuracies below support spatial consistency's connection to the model's capability. 
<img width="777" alt="image" src="https://github.com/user-attachments/assets/738a5a74-ecd0-40d5-919f-af6192c91535">


### Schematic Illustrations
- Left: Dynamic Token Morphing (DTM), Right: Overview of Masked Image Modeling via DTM
<img height="225" alt="image" src="https://github.com/user-attachments/assets/2f659939-0ed3-46b7-af5e-18c15401c355"> <img height="225" alt="image" src="https://github.com/user-attachments/assets/8a301dd1-201a-4d00-8392-78deb988364a">

## Install
Our repo is based on the following libraries
* timm==0.4.12
* pytorch==1.13

## Dataset
We pre-train our model on ImageNet-1K.

## Training and Evaluation
Detailed instructions for pre-training and fine-tuning are available in [Training.md](docs/Training.md) and [Evaluation.md](docs/Evaluation.md).


## Acknowledgements
This project is built upon [beit-v2](https://github.com/microsoft/unilm/tree/master/beit2) and borrowed features from [ToMe](https://github.com/facebookresearch/ToMe). We sincerely appreciate the authors.

## License
```
DTM
Copyright (c) 2025-present NAVER Cloud Corp.
CC BY-NC 4.0 (https://creativecommons.org/licenses/by-nc/4.0/) 
```

## Citation
If you find this repository useful, please consider citing our paper:
```
@inproceedings{
kim2025morphing,
title={Morphing Tokens Draw Strong Masked Image Models},
author={Taekyung Kim and Byeongho Heo and Dongyoon Han},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=d7q9IGj2p0}
}
```  

